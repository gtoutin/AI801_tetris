{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0659149b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99a699e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym_tetris.make('TetrisA-v2',deterministic = True)\n",
    "#env = JoypadSpace(env, MOVEMENT)\n",
    "#env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd4f3aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97bc034f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['NOOP'], ['A'], ['B'], ['right'], ['right', 'A'], ['right', 'B'], ['left'], ['left', 'A'], ['left', 'B'], ['down'], ['down', 'A'], ['down', 'B']]\n"
     ]
    }
   ],
   "source": [
    "print(MOVEMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cb88abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nickg\\anaconda3\\Lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment TetrisA-v2 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\nickg\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\nickg\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:272: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\nickg\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "C:\\Users\\nickg\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jd\n",
      "(1, 18)\n",
      "['Jd']\n",
      "Jl\n",
      "(1, 18)\n",
      "['Jd', 'Jl']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 132\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# comparison\u001b[39;00m\n\u001b[0;32m    131\u001b[0m agent \u001b[38;5;241m=\u001b[39m Tester(episodes\u001b[38;5;241m=\u001b[39mN_EPISODES)\n\u001b[1;32m--> 132\u001b[0m agent\u001b[38;5;241m.\u001b[39mplay()\n",
      "Cell \u001b[1;32mIn[1], line 114\u001b[0m, in \u001b[0;36mTester.play\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpiecesChecked\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo)\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpiecesChecked)\n\u001b[1;32m--> 114\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m    115\u001b[0m info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_tetris\n",
    "from gym_tetris.actions import MOVEMENT\n",
    "import time\n",
    "\n",
    "\n",
    "def ColorBoardtoSimpleBoard(board):\n",
    "    simpleboard = []\n",
    "    for row in board:\n",
    "        simplerow = []\n",
    "        for cell in row:\n",
    "            if cell == 239:\n",
    "                simplerow.append(0)\n",
    "            else:\n",
    "                simplerow.append(1)\n",
    "        simpleboard.append(tuple(simplerow))\n",
    "    return tuple(simpleboard)\n",
    "\n",
    "class Tester:\n",
    "    def __init__(self, episodes=100):\n",
    "        self.env = gym_tetris.make('TetrisA-v2')\n",
    "        self.env = JoypadSpace(self.env, MOVEMENT)\n",
    "        #self.env.deterministic = True\n",
    "        #Testing to see whether using the pixels for the state works better than just the board.\n",
    "        #self.state = self.env.reset()\n",
    "        self.env.reset()\n",
    "        #self.env.deterministic = True\n",
    "        self.env.render()\n",
    "        self.state = tuple([ColorBoardtoSimpleBoard(self.env.ram[0x0400:0x04C8].reshape((20, 10)).copy()), \"\", \"\", tuple(self.env.ram[0x0040:0x0042])])\n",
    "        #print(self.env.ram[0x0044])\n",
    "        \n",
    "        self.piecesChecked = []\n",
    "        self.DownLeft = 0\n",
    "        self.info = \"NONE\" \n",
    "        self.wait = 5\n",
    "        \n",
    "        self.actions = MOVEMENT\n",
    "        self.state_actions = []  # state & action track\n",
    "\n",
    "        self.episodes = episodes  # number of episodes going to play\n",
    "        self.steps_per_episode = []\n",
    "        \n",
    "        \n",
    "        \n",
    "    def chooseAction(self):\n",
    "        action = []\n",
    "        #Insert macro/micro algorithm here:\n",
    "        \n",
    "        #Right now just have the agent make random actions until we get the 2-stage algorithm working\n",
    "        #print(self.actions)\n",
    "        if self.DownLeft == 1:\n",
    "            action.append(\"down\")\n",
    "        elif self.wait <= 0:\n",
    "            action.append(\"left\")\n",
    "        if self.info in self.piecesChecked:\n",
    "            action.append(\"A\")\n",
    "        if len(action) == 0:\n",
    "            action.append(\"NOOP\")\n",
    "            \n",
    "        self.wait -= 1\n",
    "        #print(action)\n",
    "        \n",
    "        return self.actions.index(action)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.close()\n",
    "        self.env = gym_tetris.make('TetrisA-v2')\n",
    "        self.env = JoypadSpace(self.env, MOVEMENT)\n",
    "        #self.env.deterministic = True\n",
    "        self.env.reset()\n",
    "        #self.env.deterministic = True\n",
    "        self.env.render()\n",
    "        self.info = \"NONE\"\n",
    "        self.wait = 100\n",
    "        self.state = tuple([ColorBoardtoSimpleBoard(self.env.ram[0x0400:0x04C8].reshape((20, 10)).copy()), \"\", \"\", tuple(self.env.ram[0x0040:0x0042])])\n",
    "        #Conversion of self.state to tuple for hashing purposes\n",
    "        #self.state = tuple([tuple(x) for x in self.state])\n",
    "        self.state_actions = []\n",
    "\n",
    "\n",
    "    def play(self):\n",
    "        self.steps_per_episode = []  \n",
    "        \n",
    "        for ep in range(self.episodes):\n",
    "            done = False\n",
    "            while not done:\n",
    "                self.DownLeft += 1\n",
    "                if self.DownLeft > 1:\n",
    "                    self.DownLeft = 0\n",
    "                action = self.chooseAction()\n",
    "                #print(action)\n",
    "                self.state_actions.append((self.state, action))\n",
    "                try:\n",
    "                    self.info = info[\"current_piece\"]\n",
    "                    self.prevpos = tuple(self.env.ram[0x0040:0x0042])\n",
    "                except:\n",
    "                    j = \"j\"\n",
    "\n",
    "                #unusedstate, reward, done, info = self.env.step(self.env.action_space.sample())\n",
    "                #print(self.env.action_space.sample())\n",
    "                unusedstate, reward, done, info = self.env.step(action)\n",
    "                self.env.render()\n",
    "                if self.info[0] != \"J\" and self.info != \"NONE\":\n",
    "                    info = None\n",
    "                    self.reset()\n",
    "                    continue\n",
    "                #print(self.env.ram[0x0045])\n",
    "                if self.info[0] != info[\"current_piece\"][0] and self.info != \"NONE\":\n",
    "                    print(self.info)\n",
    "                    print(self.prevpos)\n",
    "                    if self.info not in self.piecesChecked:\n",
    "                        self.piecesChecked.append(self.info)\n",
    "                        print(self.piecesChecked)\n",
    "                        time.sleep(10)\n",
    "                    info = None\n",
    "                    self.reset()\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                nxtState = tuple([ColorBoardtoSimpleBoard(self.env.ram[0x0400:0x04C8].reshape((20, 10)).copy()), info[\"current_piece\"], info[\"next_piece\"], tuple(self.env.ram[0x0040:0x0042])])\n",
    "\n",
    "            # end of game\n",
    "            #if ep % 10 == 0:\n",
    "            #print(ColorBoardtoSimpleBoard(self.env.ram[0x0400:0x04C8].reshape((20, 10)).copy()))\n",
    "            #self.reset()\n",
    "        self.env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    N_EPISODES = 1\n",
    "    # comparison\n",
    "    agent = Tester(episodes=N_EPISODES)\n",
    "    agent.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2e67a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(10)\n",
    "print(\"E\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "677a1959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 1, 2: 2, 3: 128, 4: 129, 5: 130, 6: 64, 7: 65, 8: 66, 9: 32, 10: 33, 11: 34}\n"
     ]
    }
   ],
   "source": [
    "print(agent.env._action_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae03e195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
