{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary Dependencies:\n",
    "#!pip install gym_tetris\n",
    "#!pip install nes-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nickg\\anaconda3\\Lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment TetrisA-v2 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\nickg\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\nickg\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:272: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\nickg\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "C:\\Users\\nickg\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0\n",
      "Highscore: 2\n",
      "Score rate: 0.00020612181799443472\n",
      "Safety score: 9.28774605792023\n"
     ]
    }
   ],
   "source": [
    "#Necessary Dependencies:\n",
    "#pip install gym_tetris\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_tetris\n",
    "from gym_tetris.actions import MOVEMENT\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "#Takes the board ripped out from the NES RAM and converts it to a simpler board to use\n",
    "#Is mostly necessary because the board in the NES RAM also takes into account the colors of the pieces, which is not necessary for the algorithm to know.\n",
    "def ColorBoardtoSimpleBoard(board):\n",
    "    simpleboard = []\n",
    "    for row in board:\n",
    "        simplerow = []\n",
    "        for cell in row:\n",
    "            if cell == 239:\n",
    "                simplerow.append(0)\n",
    "            else:\n",
    "                simplerow.append(1)\n",
    "        simpleboard.append(tuple(simplerow))\n",
    "    return tuple(simpleboard)\n",
    "    \n",
    "        \n",
    "\n",
    "class Agent:\n",
    "    \n",
    "    def __init__(self, exp_rate=0.1, lr=0.1, n_steps=5, episodes=1):\n",
    "        self.env = gym_tetris.make('TetrisA-v2',deterministic = True)\n",
    "        self.env = JoypadSpace(self.env, MOVEMENT)\n",
    "        #self.env.deterministic = True\n",
    "        #Testing to see whether using the pixels for the state works better than just the board.\n",
    "        #self.state = self.env.reset()\n",
    "        self.env.reset()\n",
    "        #self.env.deterministic = True\n",
    "        self.env.render()\n",
    "        self.state = tuple([ColorBoardtoSimpleBoard(self.env.ram[0x0400:0x04C8].reshape((20, 10)).copy()), \"\", \"\", tuple(self.env.ram[0x0040:0x0042])])\n",
    "\n",
    "        self.highscore = 0\n",
    "        self.time = 0\n",
    "        self.linestates = []\n",
    "        self.listofhighscores = []\n",
    "        self.listofhighscorerates = []\n",
    "        self.listofsafetyscores = []\n",
    "        \n",
    "        self.actions = MOVEMENT\n",
    "        self.state_actions = []  # state & action track\n",
    "        self.exp_rate = exp_rate\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.steps = n_steps\n",
    "        self.episodes = episodes  # number of episodes going to play\n",
    "        self.steps_per_episode = []\n",
    "        \n",
    "        \n",
    "        \n",
    "    def chooseAction(self):\n",
    "        action = \"\"\n",
    "        #Insert macro/micro algorithm here:\n",
    "        \n",
    "        #Right now just have the agent make random actions until we get the 2-stage algorithm working\n",
    "        #print(self.actions)\n",
    "        action = self.actions[np.random.choice(len(self.actions))]\n",
    "        \n",
    "        \n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.close()\n",
    "        self.env = gym_tetris.make('TetrisA-v2',deterministic = True)\n",
    "        self.env = JoypadSpace(self.env, MOVEMENT)\n",
    "        #self.env.deterministic = True\n",
    "        self.env.reset()\n",
    "        #self.env.deterministic = True\n",
    "        self.env.render()\n",
    "        self.state = tuple([ColorBoardtoSimpleBoard(self.env.ram[0x0400:0x04C8].reshape((20, 10)).copy()), \"\", \"\", tuple(self.env.ram[0x0040:0x0042])])\n",
    "        #Conversion of self.state to tuple for hashing purposes\n",
    "        #self.state = tuple([tuple(x) for x in self.state])\n",
    "        self.state_actions = []\n",
    "        self.highscore = 0\n",
    "        self.time = 0\n",
    "        self.linestates = []\n",
    "\n",
    "\n",
    "    def play(self):\n",
    "        self.steps_per_episode = []  \n",
    "        \n",
    "        for ep in range(self.episodes):\n",
    "            done = False\n",
    "            while not done:\n",
    "\n",
    "                action = tuple(self.chooseAction())\n",
    "                self.state_actions.append((self.state, action))\n",
    "\n",
    "                unusedstate, reward, done, info = self.env.step(self.env.action_space.sample())\n",
    "                \n",
    "                \n",
    "                self.env.render()\n",
    "                nxtState = tuple([ColorBoardtoSimpleBoard(self.env.ram[0x0400:0x04C8].reshape((20, 10)).copy()), info[\"current_piece\"], info[\"next_piece\"], tuple(self.env.ram[0x0040:0x0042])])\n",
    "                self.highscore = info[\"score\"]\n",
    "                self.time += 1\n",
    "                self.linestates.append(info[\"board_height\"])\n",
    "\n",
    "            # end of game\n",
    "            #if ep % 10 == 0:\n",
    "            self.listofhighscores.append(self.highscore)\n",
    "            self.listofhighscorerates.append(self.highscore / self.time)\n",
    "            self.listofsafetyscores.append(sum(self.linestates) / self.time)\n",
    "            print(\"episode\", ep)\n",
    "            print(\"Highscore: \" + str(self.highscore))\n",
    "            print(\"Score rate: \" + str(self.highscore / self.time))\n",
    "            print(\"Safety score: \" + str(sum(self.linestates) / self.time))\n",
    "            self.steps_per_episode.append(len(self.state_actions))\n",
    "            self.reset()\n",
    "if __name__ == \"__main__\":\n",
    "    N_EPISODES = 30\n",
    "    # comparison\n",
    "    agent = Agent(n_steps=5, episodes=N_EPISODES)\n",
    "    agent.play()\n",
    "\n",
    "    highscores = agent.listofhighscores\n",
    "    highscorerates = agent.listofhighscorerates\n",
    "    safetyscores = agent.listofsafetyscores\n",
    "\n",
    "    plt.figure(figsize=[10, 6])\n",
    "    plt.ylim(0, 50)\n",
    "    plt.plot(range(N_EPISODES), highscores, label=\"high score\")\n",
    "    plt.legend()\n",
    "        \n",
    "    plt.figure(figsize=[10, 6])\n",
    "    plt.ylim(0, 0.1)\n",
    "    plt.plot(range(N_EPISODES), highscorerates, label=\"score rate\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.figure(figsize=[10, 6])\n",
    "    plt.ylim(4, 12)\n",
    "    plt.plot(range(N_EPISODES), safetyscores, label=\"safety score\")\n",
    "    plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
