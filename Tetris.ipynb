{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary Dependencies:\n",
    "#!pip install gym_tetris\n",
    "#!pip install nes-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary Dependencies:\n",
    "#pip install gym_tetris\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_tetris\n",
    "from gym_tetris.actions import MOVEMENT\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "#Necessary constant\n",
    "#This implementation uses the NTSC version of tetris, which has slightly different frames for the falling pieces than the PAL version.\n",
    "#Source for fall frames: https://listfist.com/list-of-tetris-levels-by-speed-nes-ntsc-vs-pal\n",
    "SpeedtoFallFrames = [48, 43, 38, 28, 23, 18, 13, 8, 6, 5, 5, 5, 4, 4, 4, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1]\n",
    "\n",
    "#This is what corresponds to the piece location/rotation, which is found via self.env.ram[0x0042].\n",
    "#It's not particularly useful for our purposes because info[\"current_piece\"] is just outright better, but it is still noteworthy.\n",
    "PieceOrientation = {\n",
    "    \"T\" : [0, 1, 2, 3],\n",
    "    \"J\" : [4, 5, 6, 7],\n",
    "    \"Z\" : [8, 9],\n",
    "    \"O\" : [10],\n",
    "    \"S\" : [11, 12],\n",
    "    \"L\" : [13, 14, 15, 16],\n",
    "    \"I\" : [17, 18]\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#Takes the board ripped out from the NES RAM and converts it to a simpler board to use\n",
    "#Is mostly necessary because the board in the NES RAM also takes into account the colors of the pieces, which is not necessary for the algorithm to know.\n",
    "def ColorBoardtoSimpleBoard(board):\n",
    "    simpleboard = []\n",
    "    for row in board:\n",
    "        simplerow = []\n",
    "        for cell in row:\n",
    "            if cell == 239:\n",
    "                simplerow.append(0)\n",
    "            else:\n",
    "                simplerow.append(1)\n",
    "        simpleboard.append(tuple(simplerow))\n",
    "    return tuple(simpleboard)\n",
    "    \n",
    "\n",
    "#To use this class, simply run the following line:\n",
    "#MicroState(ColorBoardtoSimpleBoard(self.env.ram[0x0400:0x04C8].reshape((20, 10)).copy()), info[\"current_piece\"], tuple(self.env.ram[0x0040:0x0042]), self.env.ram[0x0044])\n",
    "class MicroState:\n",
    "    \n",
    "    def __init__(self, boardstate, currentpiece, piecelocation, speed):\n",
    "        self.boardstate = boardstate\n",
    "        self.currentpiece = currentpiece\n",
    "        self.piecelocation = piecelocation\n",
    "        self.piecerotation = piecerotation\n",
    "        self.speed = speed\n",
    "        self.fallframes = SpeedtoFallFrames[min(speed, 29)]\n",
    "\n",
    "#To use this class, simply run the following line:\n",
    "##MacroState(ColorBoardtoSimpleBoard(self.env.ram[0x0400:0x04C8].reshape((20, 10)).copy()), info[\"current_piece\"], info[\"next_piece\"])\n",
    "class MacroState:\n",
    "    \n",
    "    def __init__(self, boardstate, currentpiece, nextpiece):\n",
    "        self.boardstate = boardstate\n",
    "        self.currentpiece = currentpiece\n",
    "        self.nextpiece = nextpiece\n",
    "\n",
    "class Agent:\n",
    "    \n",
    "    def __init__(self, episodes=1):\n",
    "        self.env = gym_tetris.make('TetrisA-v2',deterministic = True)\n",
    "        self.env = JoypadSpace(self.env, MOVEMENT)\n",
    "        #self.env.deterministic = True\n",
    "        #Testing to see whether using the pixels for the state works better than just the board.\n",
    "        #self.state = self.env.reset()\n",
    "        self.env.reset()\n",
    "        #self.env.deterministic = True\n",
    "        self.env.render()\n",
    "        self.state = tuple([ColorBoardtoSimpleBoard(self.env.ram[0x0400:0x04C8].reshape((20, 10)).copy()), \"\", \"\", tuple(self.env.ram[0x0040:0x0042])])\n",
    "        #print(self.env.ram[0x0044])\n",
    "        \n",
    "        self.highscore = 0\n",
    "        self.time = 0\n",
    "        self.linestates = []\n",
    "        self.listofhighscores = []\n",
    "        self.listofhighscorerates = []\n",
    "        self.listofsafetyscores = []\n",
    "        \n",
    "        self.actions = MOVEMENT\n",
    "        self.state_actions = []  # state & action track\n",
    "\n",
    "        self.episodes = episodes  # number of episodes going to play\n",
    "        self.steps_per_episode = []\n",
    "        \n",
    "        \n",
    "        \n",
    "    def chooseAction(self):\n",
    "        action = \"\"\n",
    "        #Insert macro/micro algorithm here:\n",
    "        \n",
    "        #Right now just have the agent make random actions until we get the 2-stage algorithm working\n",
    "        #print(self.actions)\n",
    "        action = self.actions[np.random.choice(len(self.actions))]\n",
    "        \n",
    "        \n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.close()\n",
    "        self.env = gym_tetris.make('TetrisA-v2',deterministic = True)\n",
    "        self.env = JoypadSpace(self.env, MOVEMENT)\n",
    "        #self.env.deterministic = True\n",
    "        self.env.reset()\n",
    "        #self.env.deterministic = True\n",
    "        self.env.render()\n",
    "        self.state = tuple([ColorBoardtoSimpleBoard(self.env.ram[0x0400:0x04C8].reshape((20, 10)).copy()), \"\", \"\", tuple(self.env.ram[0x0040:0x0042])])\n",
    "        #Conversion of self.state to tuple for hashing purposes\n",
    "        #self.state = tuple([tuple(x) for x in self.state])\n",
    "        self.state_actions = []\n",
    "        self.highscore = 0\n",
    "        self.time = 0\n",
    "        self.linestates = []\n",
    "\n",
    "\n",
    "    def play(self):\n",
    "        self.steps_per_episode = []  \n",
    "        \n",
    "        for ep in range(self.episodes):\n",
    "            done = False\n",
    "            while not done:\n",
    "\n",
    "                action = tuple(self.chooseAction())\n",
    "                self.state_actions.append((self.state, action))\n",
    "                #try:\n",
    "                #    previnfo = info[\"current_piece\"]\n",
    "                #    prevpos = tuple(self.env.ram[0x0040:0x0042])\n",
    "                #except:\n",
    "                #    j = \"j\"\n",
    "\n",
    "                unusedstate, reward, done, info = self.env.step(self.env.action_space.sample())\n",
    "                #print(self.env.ram[0x0045])\n",
    "                #try:\n",
    "                #    if previnfo[0] != info[\"current_piece\"][0]:\n",
    "                #        print(previnfo)\n",
    "                #        print(prevpos)\n",
    "                #except:\n",
    "                #    j = \"j\"\n",
    "                #print(info[\"current_piece\"])\n",
    "                #print(info[\"next_piece\"])\n",
    "                #print(self.env.ram[0x0042])\n",
    "                \n",
    "                self.env.render()\n",
    "                nxtState = tuple([ColorBoardtoSimpleBoard(self.env.ram[0x0400:0x04C8].reshape((20, 10)).copy()), info[\"current_piece\"], info[\"next_piece\"], tuple(self.env.ram[0x0040:0x0042])])\n",
    "                self.highscore = info[\"score\"]\n",
    "                self.time += 1\n",
    "                self.linestates.append(info[\"board_height\"])\n",
    "\n",
    "            # end of game\n",
    "            #if ep % 10 == 0:\n",
    "            self.listofhighscores.append(self.highscore)\n",
    "            self.listofhighscorerates.append(self.highscore / self.time)\n",
    "            self.listofsafetyscores.append(sum(self.linestates) / self.time)\n",
    "            print(\"episode\", ep)\n",
    "            print(\"Highscore: \" + str(self.highscore))\n",
    "            print(\"Score rate: \" + str(self.highscore / self.time))\n",
    "            print(\"Safety score: \" + str(sum(self.linestates) / self.time))\n",
    "            self.steps_per_episode.append(len(self.state_actions))\n",
    "            #print(ColorBoardtoSimpleBoard(self.env.ram[0x0400:0x04C8].reshape((20, 10)).copy()))\n",
    "            #self.reset()\n",
    "        #self.env.close()\n",
    "if __name__ == \"__main__\":\n",
    "    N_EPISODES = 1\n",
    "    # comparison\n",
    "    agent = Agent(episodes=N_EPISODES)\n",
    "    agent.play()\n",
    "\n",
    "    highscores = agent.listofhighscores\n",
    "    highscorerates = agent.listofhighscorerates\n",
    "    safetyscores = agent.listofsafetyscores\n",
    "\n",
    "    plt.figure(figsize=[10, 6])\n",
    "    plt.ylim(0, 50)\n",
    "    plt.plot(range(N_EPISODES), highscores, label=\"high score\")\n",
    "    plt.legend()\n",
    "        \n",
    "    plt.figure(figsize=[10, 6])\n",
    "    plt.ylim(0, 0.1)\n",
    "    plt.plot(range(N_EPISODES), highscorerates, label=\"score rate\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.figure(figsize=[10, 6])\n",
    "    plt.ylim(4, 12)\n",
    "    plt.plot(range(N_EPISODES), safetyscores, label=\"safety score\")\n",
    "    plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
